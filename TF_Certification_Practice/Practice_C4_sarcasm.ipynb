{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisyKim12/Tensorflow_Study/blob/main/Practice_C4_sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "## [중요]\n",
        "## 2023년 6월 1일 기준\n",
        "## Google Colab에 설치된 텐서플로우(TensorFlow) 버전은 2.12 버전입니다.\n",
        "## 시험에 제출 가능한 버전은 2.9.0 버전이기 때문에 버전을 다운그레이드 진행해야 합니다.\n",
        "## 반드시 아래 코드를 실행하여 버전 다운그레이드 후 모델링을 진행하세요.\n",
        "## 시험 볼 때는 현재 이 코드는 지워주셔야 합니다.\n",
        "## 관련하여 궁금하신 점은 슬랙 커뮤니티에 질문 남겨 주세요.\n",
        "####################################################\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/tfcert.py'\n",
        "urllib.request.urlretrieve(url, 'tfcert.py')\n",
        "%run tfcert.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(f'설치 완료 후 TensorFlow 버전: {tf.__version__}')\n",
        "print(f'설치 완료 후 TensorFlow Datasets 버전: {tfds.__version__}')\n",
        "\n",
        "## 시험을 위한 버전 확인 ###########\n",
        "## TensorFlow:          2.9.0 #\n",
        "## TensorFlow Datasets: 4.6.0 #\n",
        "###############################"
      ],
      "metadata": {
        "id": "_V0I0GmAK-yK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa29fa5a-0cb3-4908-ecfe-150f1228768e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "설치환경: Google Colab\n",
            "TensorFlow 시험환경을 구성중입니다. 잠시만 기다려 주세요.\n",
            "(설치는 약 1~5분 정도 소요 됩니다)\n",
            "============================================================\n",
            "============================================================\n",
            "[알림] TensorFlow 시험환경 구성이 완료 되었습니다.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설치 완료 후 TensorFlow 버전: 2.9.0\n",
            "설치 완료 후 TensorFlow Datasets 버전: 4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrt5mX8ZbiGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1547cc-1ba9-4adc-dd78-b698d9bfe72a"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 4 - sarcasm\n",
        "# val_loss: 0.3650\n",
        "# val_acc: 0.83\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "  url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "  urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "  # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "  vocab_size = 1000\n",
        "  embedding_dim = 16\n",
        "  max_length = 120\n",
        "  trunc_type='post'\n",
        "  padding_type='post'\n",
        "  oov_tok = \"<OOV>\"\n",
        "  training_size = 20000\n",
        "\n",
        "  sentences = []\n",
        "  labels = []\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  with open('sarcasm.json') as f:\n",
        "    datas = json.load(f)\n",
        "\n",
        "  for data in datas:\n",
        "    sentences.append(data['headline'])\n",
        "    labels.append(data['is_sarcastic'])\n",
        "\n",
        "  training_size = 20000\n",
        "\n",
        "  train_sentences = sentences[:training_size]\n",
        "  train_labels = labels[:training_size]\n",
        "\n",
        "  validation_sentences = sentences[training_size:]\n",
        "  validation_labels = labels[training_size:]\n",
        "\n",
        "  vocab_size = 1000\n",
        "  oov_tok = \"<OOV>\"\n",
        "  tokenizer = Tokenizer(num_words=vocab_size, oov_token = '<OOV>')\n",
        "  tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "  train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "  validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "\n",
        "  max_length = 120\n",
        "  trunc_type = 'post'\n",
        "  padding_type = 'post'\n",
        "\n",
        "  train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating = trunc_type, padding = padding_type)\n",
        "  validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "  train_labels = np.array(train_labels)\n",
        "  validation_labels = np.array(validation_labels)\n",
        "\n",
        "  embedding_dim = 16\n",
        "  model = tf.keras.Sequential([\n",
        "      Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "      Bidirectional(LSTM(64, return_sequences=True)),\n",
        "      Bidirectional(LSTM(64)),\n",
        "      Dense(256, activation='relu'),\n",
        "      Dropout(0.5),\n",
        "      Dense(128, activation='relu'),\n",
        "      Dense(64, activation ='relu'),\n",
        "      Dense(16, activation='relu'),\n",
        "\n",
        "      # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "  checkpoint_path = 'my_checkpoint.ckpt'\n",
        "  checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                              save_weights_only=True,\n",
        "                              save_best_only=True,\n",
        "                              monitor='val_loss',\n",
        "                              verbose=1)\n",
        "\n",
        "  model.fit(train_padded, train_labels,\n",
        "            validation_data=(validation_padded, validation_labels),\n",
        "            callbacks=[checkpoint],\n",
        "            epochs=10,\n",
        "            verbose = 1)\n",
        "\n",
        "  model.load_weights(checkpoint_path)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "  model = solution_model()\n",
        "  model.save(\"TF4-sarcasm.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "624/625 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.7622\n",
            "Epoch 1: val_loss improved from inf to 0.39340, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 22s 26ms/step - loss: 0.4604 - acc: 0.7623 - val_loss: 0.3934 - val_acc: 0.8226\n",
            "Epoch 2/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8427\n",
            "Epoch 2: val_loss improved from 0.39340 to 0.37560, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.3488 - acc: 0.8429 - val_loss: 0.3756 - val_acc: 0.8298\n",
            "Epoch 3/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8578\n",
            "Epoch 3: val_loss did not improve from 0.37560\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.3237 - acc: 0.8579 - val_loss: 0.3991 - val_acc: 0.8228\n",
            "Epoch 4/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.8639\n",
            "Epoch 4: val_loss improved from 0.37560 to 0.36931, saving model to my_checkpoint.ckpt\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.3094 - acc: 0.8638 - val_loss: 0.3693 - val_acc: 0.8313\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2942 - acc: 0.8731\n",
            "Epoch 5: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.2942 - acc: 0.8731 - val_loss: 0.3793 - val_acc: 0.8237\n",
            "Epoch 6/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.8763\n",
            "Epoch 6: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 13s 21ms/step - loss: 0.2874 - acc: 0.8762 - val_loss: 0.3913 - val_acc: 0.8268\n",
            "Epoch 7/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.8810\n",
            "Epoch 7: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 13s 22ms/step - loss: 0.2773 - acc: 0.8811 - val_loss: 0.3992 - val_acc: 0.8275\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2699 - acc: 0.8829\n",
            "Epoch 8: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 15s 24ms/step - loss: 0.2699 - acc: 0.8829 - val_loss: 0.3920 - val_acc: 0.8295\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - ETA: 0s - loss: 0.2617 - acc: 0.8873\n",
            "Epoch 9: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.2617 - acc: 0.8873 - val_loss: 0.4389 - val_acc: 0.8247\n",
            "Epoch 10/10\n",
            "623/625 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.8896\n",
            "Epoch 10: val_loss did not improve from 0.36931\n",
            "625/625 [==============================] - 13s 21ms/step - loss: 0.2544 - acc: 0.8894 - val_loss: 0.4443 - val_acc: 0.8268\n"
          ]
        }
      ]
    }
  ]
}
