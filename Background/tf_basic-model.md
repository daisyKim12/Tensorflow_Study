# Basic model

**content covered**
- tensor
- tf.keras.Layer
- tf.keras.Layer.Dense
- tf.keras.optimizers
    - gradient descent
    - adam
- loss function(regression)
    - L1
    - L2
    - Huber loss
- Sequential
- compile
- fit

## tensor
> What flows through layers are tensors.
A tensor is just a container for data, typically numerical data. It is, therefore, a container for numbers. Tensors are a generalization of matrices to any number of dimensions. You may already be familiar with matrices, which are 2D tensors

![Tensor](https://editor.analyticsvidhya.com/uploads/64798tensor.png)

## tf.keras.layers
[what is a layer in keras](https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc)

![neural network](https://i.stack.imgur.com/iHW2o.jpg)

For any Keras layer (Layer class) there are attribute that are important. Those are input_shape, units, activation.

### Units
> The amount of node in a layer

### input shape
> The input layer itself is not a layer, but a tensor. It's the starting tensor you send to the first hidden layer. This tensor must have the same shape as your training data.

```
30 Image
Image = (50 x 50) Pixel, RGB mode

input_data_dim = **(30, 50, 50, 3)**
input_shape = **(50, 50, 3)**
```

```
Conv2D(filters=64, kernel_size=1, input_shape=(10,10,3))
```

### activation
> Activatin funtion that makes nonlinear in every node of the layer

- sigmoid

- relu


## tf.keras.layers.Dense

```
tf.keras.layers.Dense(
    units,
    activation=None,
    ...
    **kwargs
)
```

`units`: Positive integer, dimensionality of the output space.
`activation`: Activation function to use. Usually 'relu'

*keyword args*
- Input shape
N-D tensor with shape: (batch_size, ..., input_dim). The most common situation would be a 2D input with shape (batch_size, input_dim).
- Output shape
N-D tensor with shape: (batch_size, ..., units). For instance, for a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units).

```
tf.keras.layers.Dense(128, activation='relu', input_shape =[10])
tf.keras.layers.Dense(64, activation='relu')
```

## tf.keras.optimizers
Optimizer is a algorithms that updates the weight in the model to lessen the total loss(difference between real value and predicted value)

**optimizer tensorflow provides**
- Gradient Descent
- SGD
- AdaGrad
- RMSprop
- Adadelta
- Adam
- AdaMax
- NAdam
- FTRL


**Important parameter in fine tuning**
- learning_rate: rate at which algorithm updats the parameter
    - large: Might diverge but learning speed increased
    - small: Converges but learning speed slow
- momentum:  accelerates gradient descent in appropriate direction. Helps avoid local minimum and find the global minimum.

### tf.keras.optimizer.SGD
Stochastic Gradient Descent updates weight by every single data in the whole set.

```
tf.kears.optimizers.SGD(learning_rate = 0.01,
                                momentum=0.0, 
                                nesterov=False, 
                                name='SGD', 
                                **kwargs)
```




### tf.keras.optimizers.Adam
Adaptive Moment Estimation(Adam) is among the top-most optimization techniques used today. In this method, the adaptive learning rate for each parameter is calculated. It uses Gradent Descent algorithm but enhance the performance by using momentum and decaying learning weight.

```
tf.keras.optimizers.Adam(leaarning_rate=0.001, 
                                 beta_1=0.9, 
                                 beta_2=0.999, 
                                 epsilon=1e-07, 
                                 amsgrad=False,
                                 name='Adam', 
                                 **kwargs)
```


## Loss function
Loss is the difference between real value and predicted value generated by the model. Every Network's objective is to decrease the loss in order to accurately predict the value.

### L1 loss (= MAE)
L1 loss so called mean absolute error is calculated by the absolute difference between y and f(x)

```
import numpy as np

def mse_loss(y_pred, y_true):
    n = len(y_true)
    mse_loss = np.sum(np.abs(y_pred - y_true) / n)
    return mse_loss
```

### L2 loss (=MSE)
L2 loss so called mean squared error is calculated by square the difference between y and f(x)

```
import numpy as np

def mse_loss(y_pred, y_true):
    n = len(y_true)
    mse_loss = np.sum((y_pred - y_true) ** 2) / n
    return mse_loss
```

### Huber loss
Huber loss tries to collect the advantage of L1, L2 loss. It uses L1 loss until certain threshold and uses L2 loss. Advantage of Huber loss is that it is diffentiable in every point and demonstrate a resilient personality against outliers.

