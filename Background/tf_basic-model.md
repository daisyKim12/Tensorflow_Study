# Basic model

**CONTENT COVERED**
- tensor
- tf.keras.Layer
- tf.keras.Layer.Dense
- tf.keras.optimizers
    - gradient descent
    - adam
- loss function(regression)
    - L1
    - L2
    - Huber loss
- Modeling, Compile, Fit
  - tf.keras.Sequential
  - model.compile
  - model.fit
- Epochs vs Batch
- ModelCheckpoint

## tensor
> What flows through layers are tensors.
A tensor is just a container for data, typically numerical data. It is, therefore, a container for numbers. Tensors are a generalization of matrices to any number of dimensions. You may already be familiar with matrices, which are 2D tensors

![Tensor](https://editor.analyticsvidhya.com/uploads/64798tensor.png)

## tf.keras.layers
[what is a layer in keras](https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc)

![neural network](https://i.stack.imgur.com/iHW2o.jpg)

For any Keras layer (Layer class) there are attribute that are important. Those are input_shape, units, activation.

### Units
> The amount of node in a layer

### input shape
> The input layer itself is not a layer, but a tensor. It's the starting tensor you send to the first hidden layer. This tensor must have the same shape as your training data.

```
30 Image
Image = (50 x 50) Pixel, RGB mode

input_data_dim = **(30, 50, 50, 3)**
input_shape = **(50, 50, 3)**
```

```
Conv2D(filters=64, kernel_size=1, input_shape=(10,10,3))
```

### activation
> Activatin funtion that makes nonlinear in every node of the layer

- sigmoid

- relu


## tf.keras.layers.Dense

```
tf.keras.layers.Dense(
    units,
    activation=None,
    ...
    **kwargs
)
```

`units`: Positive integer, dimensionality of the output space.
`activation`: Activation function to use. Usually 'relu'

*keyword args*
- Input shape
N-D tensor with shape: (batch_size, ..., input_dim). The most common situation would be a 2D input with shape (batch_size, input_dim).
- Output shape
N-D tensor with shape: (batch_size, ..., units). For instance, for a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units).

```
tf.keras.layers.Dense(128, activation='relu', input_shape =[10])
tf.keras.layers.Dense(64, activation='relu')
```

## tf.keras.optimizers
> Optimizer is a algorithms that updates the weight in the model to lessen the total loss(difference between real value and predicted value)

**optimizer tensorflow provides**
- Gradient Descent
- SGD
- AdaGrad
- RMSprop
- Adadelta
- Adam
- AdaMax
- NAdam
- FTRL


**Important parameter in fine tuning**
- learning_rate: rate at which algorithm updats the parameter
    - large: Might diverge but learning speed increased
    - small: Converges but learning speed slow
- momentum:  accelerates gradient descent in appropriate direction. Helps avoid local minimum and find the global minimum.

### tf.keras.optimizer.SGD
Stochastic Gradient Descent updates weight by every single data in the whole set.

```
tf.kears.optimizers.SGD(learning_rate = 0.01,
                                momentum=0.0, 
                                nesterov=False, 
                                name='SGD', 
                                **kwargs)
```




### tf.keras.optimizers.Adam
Adaptive Moment Estimation(Adam) is among the top-most optimization techniques used today. In this method, the adaptive learning rate for each parameter is calculated. It uses Gradent Descent algorithm but enhance the performance by using momentum and decaying learning weight.

```
tf.keras.optimizers.Adam(leaarning_rate=0.001, 
                                 beta_1=0.9, 
                                 beta_2=0.999, 
                                 epsilon=1e-07, 
                                 amsgrad=False,
                                 name='Adam', 
                                 **kwargs)
```


## Loss function
> Loss is the difference between real value and predicted value generated by the model. Every Network's objective is to decrease the loss in order to accurately predict the value.

### L1 loss (= MAE)
L1 loss so called mean absolute error is calculated by the absolute difference between y and f(x)

```
import numpy as np

def mse_loss(y_pred, y_true):
    n = len(y_true)
    mse_loss = np.sum(np.abs(y_pred - y_true) / n)
    return mse_loss
```

### L2 loss (=MSE)
L2 loss so called mean squared error is calculated by square the difference between y and f(x)

```
import numpy as np

def mse_loss(y_pred, y_true):
    n = len(y_true)
    mse_loss = np.sum((y_pred - y_true) ** 2) / n
    return mse_loss
```

### Huber loss
> Huber loss tries to collect the advantage of L1, L2 loss. It uses L1 loss until certain threshold and uses L2 loss. Advantage of Huber loss is that it is diffentiable in every point and demonstrate a resilient personality against outliers.


## Modeling, Compile, Fit

### tf.keras.Sequential
> Sequential groups a linear stack of layers into a tf.keras.Model.
Sequential provides training and inference features on this model.

```
tf.keras.Sequential(layers=None, name=None)

```

**additional method**
1. add
Adds a layer instance on top of the layer stack.
```
Sequential.add(layer)
```

2. pop
Removes the last layer in the model.
```
Sequential.pop()
```

### model.compile
> `compile` method in model configures the model for training. Meaning `compile` method saves the options needed for traing the model.
```
Model.compile(
    optimizer="rmsprop",
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    steps_per_execution=None,
    jit_compile=None,
    pss_evaluation_shards=0,
    **kwargs
)
```

**example**
```
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['acc'])
```

### fit
> Trains the model using the comfiuration `compile` method have set. And `fit` method add some more options for traing the model such and epochs(dataset iterations).
```
Model.fit(
    x=None,
    y=None,
    batch_size=None,
    epochs=1,
    verbose="auto",
    callbacks=None,
    validation_split=0.0,
    validation_data=None,
    shuffle=True,
    class_weight=None,
    sample_weight=None,
    initial_epoch=0,
    steps_per_epoch=None,
    validation_steps=None,
    validation_batch_size=None,
    validation_freq=1,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
)
```
**Argemnets**
- `x`: Input data. It could be:
  - Numpy array, or a list of arrays.
  - TensorFlow tensor, or a list of tensors.
  - tf.data dataset which returns a tuple.
- `y`: Target data. 
  - Numpy array, or a list of arrays.
  - TensorFlow tensor, or a list of tensors.
  - tf.data dataset which returns a tuple.
- `batch_size`: Integer or None. Number of samples per gradient update.
- `epochs`: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.
- `verbose`: Options that sets logging settings. 'auto', 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.
- `callbacks``: List of callbacks to apply during training.

## Epochs vs Batch
- Epochs: One Epoch is when an **ENTIRE** dataset is passed forward and backward through the neural network only **ONCE**

- Batch: Batch size is a number of samples processed before the model is updated. So Batch is a **sub-group** of **ENTIRE** dataset. Weight of a network is updated by every single batch training.

## ModelCheckpoint
> `ModelCheckpoint` is a callback to save the Keras model or model weights by certain criterion.

ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.

```
checkpoint_path = "my_checkpoint.ckpt"
checkpoint = ModelCheckpoint(filepath=checkpoint_path,
                             save_weights_only = True,
                             save_best_only = True,
                             monitor = 'val_loss',
                             verbose=1)
```

__Caution__ Make sure after learning process is finished load_weight from checkpoint to load the weight checkpoint is saving, which is the weight of the lowest val_loss

```
model.load_weights(checkpoint_path)
```